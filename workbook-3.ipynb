{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jtallison/LDLFest-workshop/blob/master/workbook-3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with a Corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assembling Your Own Datasets\n",
    "\n",
    "Places to get stuff:\n",
    "\n",
    "* Project Gutenberg: (http://gutenberg.org/)\n",
    "* Google Books (http://books.google.com)\n",
    "* EEBO (http://quod.lib.umich.edu/e/eebogroup/)\n",
    "* ECCO (http://quod.lib.umich.edu/e/ecco/)\n",
    "* Evans (http://quod.lib.umich.edu/e/evans/)\n",
    "* JSTOR DFR (http://dfr.jstor.org/)\n",
    "* Open Access: PMC Open Access Set, PLoS, BioMed Central\n",
    "* _Mining the Social Web_ (O'Reilly)\n",
    "* Twitter APIs (http://dev.twitter.com)\n",
    "* Facebook APIs (http://developers/facebook.com)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Troubles with Access and Quality\n",
    "\n",
    "The elephant in the room is copyright. For-profit journals: Elsevier has a text-mining API; otherwise negotiation contracts. The same holds true for contemporary books: getting access can be difficult, even with HathiTrust.\n",
    "\n",
    "Google Ngram brings up other issues: the digitization of available materials is not complete, which suggests that the statistical significance is limited, even in contemporary English. Worries about breadth of corpus, especially in early works. Libraries scan the books they have, and libraries tend to have (still) medical and scientific volumes. E.g., any signal you get could be weak or just plain wrong. OCR noise can run wildly high.\n",
    "\n",
    "re: Social media, see: Boyd and Crawford 2011 (SSRN: 1926431).\n",
    "\n",
    "Not only is OCR problematic, but automated tasks, like named entity extraction, are also questionable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting Texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `wget`\n",
    "\n",
    "Sometimes CLI tools, like `wget`, are more powerful than GUI tools. The key difference is that GUI tools are easier to use at first, but repetitive tasks are difficult or expensive (in terms of time). CLI tools are a little more difficult at first, but once you have an established collection of them, they are not only easier to use but just plain easier. \n",
    "\n",
    "**`wget`** is one of those tools. E.g.:\n",
    "\n",
    "    % wget -r -l 1 -w 2 --limit-rate=20k https://www.cs.cmu.edu/~spok/grimmtmp/\n",
    "\n",
    "`wget` is a CLI program that retrieves web content. To my mind, since it can act like a targeted web crawler, it is the single greatest tool available to those looking to gather data/texts. \n",
    "\n",
    "Let's look at what it looks like:\n",
    "\n",
    "    % wget -r -l 1 -w 2 --limit-rate=20k https://www.cs.cmu.edu/~spok/grimmtmp/\n",
    "    \n",
    "* `-r` (or `--recursive`) turns on recursive retrieving (up to 5 directories deep). \n",
    "* `-l 1` (or`--level=1`) keeps the depth to 1.\n",
    "* `-w 2` gives the amount of time to wait between retrievals. (Two seconds lessens the server load.)\n",
    "* `--limit-rate=20k` sets the retrieval rate to 20kB/s. (This is being polite in a shared connection setting.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 1: Downloading Files\n",
    "\n",
    "http://digital.library.okstate.edu/kappler/Vol2/Toc.htm. \n",
    "\n",
    "As it turns out, almost all the documents in which we are interested are housed in a single directory (below), which does not like being crawled. Running `wget` returns **ERROR 403: Forbidden**. In all likelihood, this is the result of the site's administrator configuring the website to make sure that directories cannot be browsed directly.\n",
    "\n",
    "    !wget -r -l 1 -w 2 --limit-rate=20k http://digital.library.okstate.edu/kappler/Vol2/treaties/\n",
    "\n",
    "We need, then, to be able to access the table of contents above, get all the links listed, and then download that list into a directory (folder in GUI terms) of our choosing.\n",
    "\n",
    "While `wget` is a truly useful program, especially since one line can do so much, it does have its limitations. There are ways around it that would allow you to remain within the Bash shell, but it is also possible to replicate the power of `wget` in Python, and once you are using Python, you can do so much more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# To use this script, the user needs to provide the three values below: \n",
    "# myurl, myfilter, mydirectory\n",
    "# Please make sure `mydirectory` is already created before running\n",
    "\n",
    "myurl = \"http://digital.library.okstate.edu/kappler/Vol2/Toc.htm\"\n",
    "myfilter = \"http://digital.library.okstate.edu/kappler/Vol2/treaties/\"\n",
    "mydirectory = \"/Users/jjl/Desktop/downloadedfiles/\"\n",
    "\n",
    "myconnection = urllib.request.urlopen(myurl)\n",
    "myhtml = myconnection.read()\n",
    "mysoup = BeautifulSoup(myhtml, \"lxml\")\n",
    "mylinks = mysoup.find_all('a')\n",
    "\n",
    "all_links = []\n",
    "for tag in mylinks:\n",
    "    link = tag.get('href',None)\n",
    "    if link is not None:\n",
    "        all_links.append(link)\n",
    "\n",
    "myresults = [k for k in all_links if myfilter in k]\n",
    "\n",
    "for result in myresults:\n",
    "    remotefile = urllib.request.urlopen(result)\n",
    "    localfile = open(mydirectory+result.replace(myfilter, ''),'wb')\n",
    "    localfile.write(remotefile.read())\n",
    "    localfile.close()\n",
    "    remotefile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have a directory (folder) sitting on our desktop and it has all the files we want:\n",
    "\n",
    "![Screenshot of Full Directory](./images/Screenshot_directory.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 2: Working with an Content Management System's API\n",
    "\n",
    "What happens when the texts with which you want to work are not sitting in a directory, but are in a content management system (CMS)? Our next example was suggested by a participant who is interested in working with Paul Laurence Dunbar's poetry and fiction. Using the previous script as a basis for doing similar work, we are going to examine the URLs generated by the CMS to see if there is a way for us to get what is wanted.\n",
    "\n",
    "Here is the link for the digital archive of Dunbar’s work at Wright State: http://www.libraries.wright.edu/special/dunbar/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Screenshot of Dunbar Archive Web Page](./images/ScreenShot_Dunbar.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we click on the \"poetry\" link in the lefthand navigation pane, and then hover over one of the books (see image above), we see the following URL: \n",
    "\n",
    "    http://www.libraries.wright.edu/special/dunbar/explore?book=8\n",
    "\n",
    "Clicking on a book, takes us to a table of contents, with a series of links like this:\n",
    "\n",
    "    http://www.libraries.wright.edu/special/dunbar/explore?book=9&id=236\n",
    "\n",
    "The `id`s are not sequential within a book; however, by playing with the URLs in a browser, it looks like you can insert an asterisk into portion of the URL that identifies the book, `book=*`, and still get back results on simply the `id=`:\n",
    "\n",
    "    http://www.libraries.wright.edu/special/dunbar/explore?book=*&id=99\n",
    "\n",
    "In fact, after a little experimentation of just typing in numbers and changing the `id` number and getting back results, it looks like we just need to iterate through all the `id`s. If we start with `1`, how far up do we need to go? Since I saw numbers in the 300s earlier, I am going to start with 400 and go up by 100 until I get no results and then narrow by 10s and then 1s until I know where to stop ... and it appears we stop at 433.\n",
    "\n",
    "Now let's go build, er, revise us some code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "baseurl = \"http://www.libraries.wright.edu/special/dunbar/explore?book=*&id=\"\n",
    "mydirectory = \"/Users/jjl/Desktop/downloadedfiles/\"\n",
    "\n",
    "mylist = []\n",
    "for i in range (1, 434):\n",
    "    link = baseurl+str(i)\n",
    "    mylist.append(link)\n",
    "\n",
    "for link in mylist:\n",
    "    remotefile = urllib.request.urlopen(link).read()\n",
    "    soup = BeautifulSoup(remotefile, \"lxml\")\n",
    "    div = soup.find('div', 'bookContain-right')\n",
    "    localfile = open(mydirectory+link.replace(baseurl, '')+\".html\",'wt')\n",
    "    localfile.write(str(div.encode('utf-8')))\n",
    "    localfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code works, and it returns only the contents of the desired `div`:\n",
    "\n",
    "    <div class=\"bookContain-right\">\n",
    "\n",
    "But the contents remain ugly. At the very least, some regex is needed to clean up some of the escaped characters: those that begin with a backslash. Perhaps better would be to use `html2text` to convert the documents to plain text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "baseurl = \"http://www.libraries.wright.edu/special/dunbar/explore?book=*&id=\"\n",
    "mydirectory = \"/Users/jjl/Desktop/downloadedfiles/\"\n",
    "\n",
    "mylist = []\n",
    "for i in range (1, 2):\n",
    "    link = baseurl+str(i)\n",
    "    mylist.append(link)\n",
    "\n",
    "for link in mylist:\n",
    "    remotefile = urllib.request.urlopen(link).read()\n",
    "    soup = BeautifulSoup(remotefile, \"lxml\")\n",
    "    div = soup.find('div', 'bookContain-right')\n",
    "    text = html2text.html2text(str(div))\n",
    "    localfile = open(mydirectory+link.replace(baseurl, '')+\".txt\",'wt')\n",
    "    localfile.write(str(text))\n",
    "    localfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Munging\n",
    "\n",
    "**Data munging** or **data wrangling** is loosely the process of manually converting or mapping data from one \"raw\" form into another format that allows for more convenient consumption of the data with the help of semi-automated tools. This may include further munging, data visualization, data aggregation, training a statistical model, as well as many other potential uses. Data munging as a process typically follows a set of general steps which begin with extracting the data in a raw form from the data source, \"munging\" the raw data using algorithms (e.g. sorting) or parsing the data into predefined data structures, and finally depositing the resulting content into a data sink for storage and future use.\n",
    "\n",
    "-- [Wikipedia](https://en.wikipedia.org/wiki/Data_wrangling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "   <head>\n",
      "      <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
      "   \n",
      "      <link rel=\"stylesheet\" type=\"text/css\" href=\"kstyles.css\">\n",
      "      <title>INDIAN AFFAIRS: LAWS AND TREATIES. Vol. 2, Treaties</title>\n",
      "      <meta name=\"description\" content=\"Indian Affairs: Laws and Treaties, compi \bled and edited by  Charles J. Kappler is an historically significant, seven volu \bme compilation of U.S. treaties, laws and executive  orders pertaining to Native \b American Indian tribes. The volumes cover U.S. Government treaties  with Native \b Americans from 1778-1883 (Volume II) and U.S. laws and executive orders concern \bing Native Americans  from 1871-1970 (Volumes I, III-VII). The work was first pu \bblished in 1903-04 by the U.S. Government Printing Office.\">\n",
      "      <meta name=\"keywords\" content=\"kappler native american indian tribes treat \bies laws executive orders\">\n",
      "      <meta name=\"author\" content=\"Oklahoma State University Library\">\n",
      "   </head>\n",
      "   <body background=\"images/kappler40.gif\">\n",
      "      <h3>INDIAN AFFAIRS: LAWS AND TREATIES</h3>\n",
      "      <h5>Vol. II, Treaties &nbsp; &nbsp;</h5>\n",
      "      <p>Compiled and edited by Charles J. Kappler.\n",
      "         Washington : Government Printing Office, 1904.\n",
      "      </p>\n",
      "      <hr width=\"95%\" align=\"right\">\n",
      "      <div class=\"SANSLINE\" style=\"margin-left: 40px\"><a href=\"../../index.htm\"> \bHome</a> | <a href=\"../../../terms.html\">Disclaimer &amp; Usage</a> | <a href=\". \b./toc.htm\">Table of Contents</a> | <a href=\"../Kindex.htm\">Index</a></div>\n",
      "      <hr width=\"95%\" align=\"right\">\n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      <h4>\n",
      "         <center>TREATY WITH THE APACHE, 1852</center><br>\n",
      "         \n",
      "\u001b[Kar. 23, 1853.&nbsp;|&nbsp;Proclaimed Mar. 25, 1853.</center><br>\n",
      ":\u001b[K"
     ]
    }
   ],
   "source": [
    "# Let's take a look at one of Zach's files:\n",
    "\n",
    "!less ./texts/apa0598.htm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No matter what Zach has in mind for this data, we can be pretty sure that it does not include a lot of angle brackets and funkiness like `div class=\"SANSLINE\"`. (For the record, *funkiness* is a technical term in data munging. I'm serious. Go look it up.) Whatever Zach's next steps are, he is going to want to clean up the text. \n",
    "\n",
    "For this workshop, we are going to skip transforming this html into some kind of operable xml and focus on simply getting it into useful plain text. From there, Zach will be able to engage a number of automated processes which may be more, or less, interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "INDIAN AFFAIRS: LAWS AND TREATIES. Vol. 2, Treaties\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INDIAN AFFAIRS: LAWS AND TREATIES\n",
      "Vol. II, Treaties    \n",
      "Compiled and edited by Charles J. Kappler.\n",
      "         Washington : Government Printing Office, 1904.\n",
      "      \n",
      "\n",
      "Home | Disclaimer & Usage | Table of Contents | Index\n",
      "\n",
      "\n",
      "TREATY WITH THE APACHE, 1852\n",
      "July 1, 1852. | 10 Stat., 979. | Ratified Mar. 23, 1853. | Proclaimed Mar. 25, 1853.\n",
      "Page Images:  Page 598\n",
      "             | 599\n",
      "             | 600\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Margin Notes\n",
      "\n",
      "\n",
      "Authority of the United States acknowledged.\n",
      "\n",
      "\n",
      "Peace to exist.\n",
      "\n",
      "\n",
      "The Apaches not to assist other tribes in hostilities.\n",
      "\n",
      "\n",
      "Good treatment of citizens of the United States and nations at peace with them.\n",
      "\n",
      "\n",
      "Cases of aggression on them to be referred to Government.\n",
      "\n",
      "\n",
      "Laws to be conformed to.\n",
      "\n",
      "\n",
      "Provisions against incursions into Mexico.\n",
      "\n",
      "\n",
      "Persons injuring the Apaches to be tried and punished.\n",
      "\n",
      "\n",
      "Free passage over the Apache territory.\n",
      "\n",
      "\n",
      "Military posts, agencies, and trading houses to be established.\n",
      "\n",
      "\n",
      "Territorial boundaries to be established.\n",
      "\n",
      "\n",
      "Presents to the Apaches.\n",
      "\n",
      "\n",
      "When treaty to be binding.\n",
      "\n",
      "\n",
      "How construed.\n",
      "\n",
      "\n",
      "\n",
      "Page 598\n",
      "\n",
      "Articles of a treaty made and entered into at Santa Fe, New Mexico, on the first day of July in the year of our Lord one thousand\n",
      "            eight hundred and fifty-two, by and between Col. E. V. Sumner, U. S. A., commanding the 9th Department and in charge of the\n",
      "            executive office of New Mexico, and John Greiner, Indian agent in and for the Territory of New Mexico, and acting superintendent\n",
      "            of Indian affairs of said Territory, representing the United States, and Cuentas, Azules, Blancito, Negrito, Capitan Simon,\n",
      "            Captain Vuelta, and Mangus Colorado, chiefs, acting on the part of the Apache Nation of Indians, situate and living within\n",
      "            the limits of the United States.\n",
      "\n",
      "ARTICLE 1.\n",
      "Said nation or tribe of Indians through their authorized Chiefs aforesaid do hereby acknowledge  and declare that they are lawfully and exclusively under the laws, jurisdiction, and government of the United\n",
      "         States of America, and to its power and authority they do hereby submit.\n",
      "      \n",
      "ARTICLE 2.\n",
      "From and after the signing of this Treaty hostilities  between the contracting parties shall forever cease, and perpetual\n",
      "         peace and amity shall forever exist  between said Indians and the Government and people of the United States; the said nation, or tribe of Indians, hereby binding themselves most solemnly never to associate\n",
      "      \n",
      "\n",
      "Page 599\n",
      "with or give countenance or aid to any tribe or band of Indians, or other persons or powers, who may be at any time at war\n",
      "         or enmity with the government or people of said United States.\n",
      "      \n",
      "ARTICLE 3.\n",
      "Said nation, or tribe of Indians, do hereby bind themselves for all future time to treat honestly and humanely all citizens of the United States,  with whom they have intercourse, as well as all persons and powers, at peace with the said\n",
      "         United States, who may be lawfully among them, or with whom they may have any lawful intercourse.\n",
      "      \n",
      "ARTICLE 4.\n",
      "All said nation, or tribe of Indians, hereby bind themselves to refer all cases of aggression  against themselves or their property and territory, to the government of the United States for adjustment,\n",
      "         and to conform  in all things to the laws, rules, and regulations of said government in regard to the Indian tribes.\n",
      "      \n",
      "ARTICLE 5.\n",
      "Said nation, or tribe of Indians, do hereby bind themselves for all future time to desist and refrain from making any “incursions\n",
      "         within the Territory of Mexico” of a hostile or predatory character; and that they will for the future refrain from taking and conveying into captivity\n",
      "         any of the people or citizens of Mexico, or the animals or property of the people or government of Mexico; and that they will,\n",
      "         as soon as possible after the signing of this treaty, surrender to their agent all captives now in their possession.\n",
      "      \n",
      "ARTICLE 6.\n",
      "Should any citizen of the United States, or other person or persons subject  to the laws of the United States, murder, rob, or otherwise maltreat any Apache Indian or Indians, he or\n",
      "         they shall be arrested and tried, and upon conviction, shall be subject to all the penalties provided by law for the protection\n",
      "         of the persons and property of the people of the said States.\n",
      "      \n",
      "ARTICLE 7.\n",
      "The people of the United States of America shall have free and safe passage  through the territory of the aforesaid Indians, under such rules and regulations as may be adopted by authority of\n",
      "         the said States.\n",
      "      \n",
      "ARTICLE 8.\n",
      "In order to preserve tranquility and to afford protection to all the people and interests of the contracting parties, the\n",
      "         government of the United States of America will establish such military posts and agencies,  and authorize such trading houses at such times and places as the said government may designate.\n",
      "      \n",
      "ARTICLE 9.\n",
      "Relying confidently upon the justice and the liberality of the aforesaid government, and anxious to remove every possible\n",
      "         cause that might disturb their peace and quiet, it is agreed by the aforesaid Apache's that the government of the United States\n",
      "         shall at its earliest convenience designate, settle, and adjust their territorial boundaries,  and pass and execute in their territory such laws as may be deemed conducive to the prosperity and\n",
      "         happiness of said Indians.\n",
      "      \n",
      "ARTICLE 10.\n",
      "For and in consideration of the faithful performance of all the stipulations herein contained, by the said Apache's  Indians, the government of the United States will grant to said Indians such donations, presents, and implements,\n",
      "         and adopt such other liberal and humane measures as said government may deem meet and proper.\n",
      "      \n",
      "ARTICLE 11.\n",
      "\n",
      "         This Treaty shall be binding  upon the contracting parties from and after the signing of the same, subject only to such modifications and amendments\n",
      "         as may be adopted by the government of the United States; and, finally, this treaty is to receive a liberal construction , at all times and in all places, to the end that the said Apache Indians shall not be held responsible\n",
      "         for the conduct of others, and that the government of the United States shall so legislate and act as to secure the permanent\n",
      "         prosperity and happiness of said Indians.\n",
      "         \n",
      "      \n",
      "In faith whereof we the undersigned have signed this Treaty, and affixed thereunto our seals, at the City of Santa Fé, this\n",
      "         the first day\n",
      "Page 600\n",
      "         \n",
      "         of July in the year of our Lord one thousand eight hundred and fifty-two.\n",
      "      \n",
      "E. V. Sumner, [SEAL.]\n",
      "Bvt. Col. U. S. A. commanding Ninth Department\n",
      "               In charge of Executive Office of New Mexico.\n",
      "John Greiner, [SEAL.]Act. Supt. Indian Affairs, New Mexico.\n",
      "Capitan Vuelta, his x mark [SEAL.]\n",
      "Cuentas Azules, his x mark [SEAL.]\n",
      "Blancito ——, his x mark [SEAL.]\n",
      "Negrito ——, his x mark [SEAL.]\n",
      "Capitan Simon, his x mark [SEAL.]\n",
      "Mangus Colorado, his x mark [SEAL.]\n",
      "Witnesses:\n",
      "F. A. Cunningham, Paymaster, U. S. A.\n",
      "J. C. McFerran, 1st Lt. 3d Inf. Act. Ast. Adj. Gen.\n",
      "Caleb Sherman.\n",
      "Fred. Saynton.\n",
      "Chas. McDougall, Surgeon, U. S. A.\n",
      "S. M. Baird.\n",
      "Witness to the signing of Mangus Colorado:\n",
      "John Pope, Bvt. Capt. T. E.\n",
      "\n",
      "Search | OSU Library Electronic Publishing Center\n",
      "\n",
      "Produced by the Oklahoma State University Library\n",
      "         URL: http://digital.library.okstate.edu/kappler/\n",
      "         Comments to:  lib-dig@okstate.edu\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "myfile = open('./texts/apa0598.htm', 'r')\n",
    "myhtml = myfile.read()\n",
    "mytext = BeautifulSoup(myhtml).text\n",
    "\n",
    "print(mytext)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we just need to clean up the entire folder!\n",
    "\n",
    "Again, a bit of automation goes a long way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Please note that this script is incomplete for now. \n",
    "# Feel free to use it as a basis for a script that works.\n",
    "\n",
    "filesIN = \"/Users/jjl/Desktop/filesIN/\"\n",
    "filesOUT = \"/Users/jjl/Desktop/filesOUT/\"\n",
    "\n",
    "postlist = os.listdir(filesIN)\n",
    "\n",
    "for post in postlist: \n",
    "    text = BeautifulSoup(open(filesIN+post), \"lxml\")\n",
    "    text.encode(\"utf-8\")\n",
    "    fout = open(filesOUT+post, \"w\")\n",
    "    fout.write(text.encode(\"utf-8\"))\n",
    "    fout.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
