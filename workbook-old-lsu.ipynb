{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Ignore this\n",
    "!jupyter nbconvert 'workbook-1-intro.ipynb' --to slides --post serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Some initial magic\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "pylab.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Who's Afraid of ChatGPT\n",
    "\n",
    "## *A Gentle Introduction to Text Analytics*\n",
    "\n",
    "John Laudun  \n",
    "Department of English  \n",
    "University of Louisiana at Lafayette  \n",
    "johnlaudun@gmail.com  \n",
    "https://johnlaudun.net/  \n",
    "@johnlaudun@hcommons.social  \n",
    "\n",
    "All of today's materials, and this notebook, are available at: http://github.com/johnlaudun/workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. Getting Texts\n",
    "2. Munging Texts\n",
    "3. Text Mining with the NLTK\n",
    "4. A Brief Excursis on Control Logic\n",
    "5. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `wget`\n",
    "\n",
    "Sometimes CLI tools, like `wget`, are more powerful than GUI tools. The key difference is that GUI tools are easier to use at first, but repetitive tasks are difficult or expensive (in terms of time). CLI tools are a little more difficult at first, but once you have an established collection of them, they are not only easier to use but just plain easier. \n",
    "\n",
    "**`wget`** is one of those tools. E.g.:\n",
    "\n",
    "    % wget -r -l 1 -w 2 --limit-rate=20k https://www.cs.cmu.edu/~spok/grimmtmp/\n",
    "\n",
    "`wget` is a CLI program that retrieves web content. To my mind, since it can act like a targeted web crawler, it is the single greatest tool available to those looking to gather data/texts. \n",
    "\n",
    "Let's look at what it looks like:\n",
    "\n",
    "    % wget -r -l 1 -w 2 --limit-rate=20k https://www.cs.cmu.edu/~spok/grimmtmp/\n",
    "    \n",
    "* `-r` (or `--recursive`) turns on recursive retrieving (up to 5 directories deep). \n",
    "* `-l 1` (or`--level=1`) keeps the depth to 1.\n",
    "* `-w 2` gives the amount of time to wait between retrievals. (Two seconds lessens the server load.)\n",
    "* `--limit-rate=20k` sets the retrieval rate to 20kB/s. (This is being polite in a shared connection setting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Case 1\n",
    "\n",
    "http://digital.library.okstate.edu/kappler/Vol2/Toc.htm. \n",
    "\n",
    "As it turns out, almost all the documents in which we are interested are housed in a single directory (below), which does not like being crawled. Running `wget` returns **ERROR 403: Forbidden**. In all likelihood, this is the result of the site's administrator configuring the website to make sure that directories cannot be browsed directly.\n",
    "\n",
    "    !wget -r -l 1 -w 2 --limit-rate=20k http://digital.library.okstate.edu/kappler/Vol2/treaties/\n",
    "\n",
    "We need, then, to be able to access the table of contents above, get all the links listed, and then download that list into a directory (folder in GUI terms) of our choosing.\n",
    "\n",
    "While `wget` is a truly useful program, especially since one line can do so much, it does have its limitations. There are ways around it that would allow you to remain within the Bash shell, but it is also possible to replicate the power of `wget` in Python, and once you are using Python, you can do so much more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 8] nodename nor servname provided, or not known>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1350\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1351\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 948\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    949\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mz/k5p3f5wj0czgd_5m4dw59py00000gn/T/ipykernel_29067/3662080741.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmydirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/jjl/Desktop/downloadedfiles/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmyconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyurl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmyhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmysoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 543\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/TED/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1351\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 8] nodename nor servname provided, or not known>"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# To use this script, the user needs to provide the three values below: \n",
    "# myurl, myfilter, mydirectory\n",
    "# Please make sure `mydirectory` is already created before running\n",
    "\n",
    "myurl = \"http://digital.library.okstate.edu/kappler/Vol2/Toc.htm\"\n",
    "myfilter = \"http://digital.library.okstate.edu/kappler/Vol2/treaties/\"\n",
    "mydirectory = \"/Users/jjl/Desktop/downloadedfiles/\"\n",
    "\n",
    "myconnection = urllib.request.urlopen(myurl)\n",
    "myhtml = myconnection.read()\n",
    "mysoup = BeautifulSoup(myhtml, \"lxml\")\n",
    "mylinks = mysoup.find_all('a')\n",
    "\n",
    "all_links = []\n",
    "for tag in mylinks:\n",
    "    link = tag.get('href',None)\n",
    "    if link is not None:\n",
    "        all_links.append(link)\n",
    "\n",
    "myresults = [k for k in all_links if myfilter in k]\n",
    "\n",
    "for result in myresults:\n",
    "    remotefile = urllib.request.urlopen(result)\n",
    "    localfile = open(mydirectory+result.replace(myfilter, ''),'wb')\n",
    "    localfile.write(remotefile.read())\n",
    "    localfile.close()\n",
    "    remotefile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To work with this script, all you need to do is provide three variables:\n",
    "\n",
    "**myurl** is the page that lists the files\n",
    "\n",
    "**myfilter** is the (remote) directory in which the files are stored\n",
    "\n",
    "**mydirectory** is the (local) directory to which you wish to save the files. N.B.: You must create this directory ahead of time; and you must provide the absolute path from the root of your OS. The example path is from a Mac; Windows users will need something like `C:\\Documents and Setting\\username\\path\\to\\folder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we have a directory (folder) sitting on our desktop and it has all the files we want:\n",
    "\n",
    "![Screenshot of Full Directory](./images/Screenshot_directory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2\n",
    "\n",
    "What happens when the texts with which you want to work are not sitting in a directory, but are in a content management system (CMS)? Our next example was suggested by a participant who is interested in working with Paul Laurence Dunbar's poetry and fiction. Using the previous script as a basis for doing similar work, we are going to examine the URLs generated by the CMS to see if there is a way for us to get what is wanted.\n",
    "\n",
    "Here is the link for the digital archive of Dunbarâ€™s work at Wright State: http://www.libraries.wright.edu/special/dunbar/\n",
    "\n",
    "![Screenshot of Dunbar Archive Web Page](./images/ScreenShot_Dunbar.png)\n",
    "\n",
    "If we click on the \"poetry\" link in the lefthand navigation pane, and then hover over one of the books (see image above), we see the following URL: \n",
    "\n",
    "    http://www.libraries.wright.edu/special/dunbar/explore?book=8\n",
    "\n",
    "Clicking on a book, takes us to a table of contents, with a series of links like this:\n",
    "\n",
    "    http://www.libraries.wright.edu/special/dunbar/explore?book=9&id=236\n",
    "\n",
    "The `id`s are not sequential within a book; however, by playing with the URLs in a browser, it looks like you can insert an asterisk into portion of the URL that identifies the book, `book=*`, and still get back results on simply the `id=`:\n",
    "\n",
    "    http://www.libraries.wright.edu/special/dunbar/explore?book=*&id=99\n",
    "\n",
    "In fact, after a little experimentation of just typing in numbers and changing the `id` number and getting back results, it looks like we just need to iterate through all the `id`s. If we start with `1`, how far up do we need to go? Since I saw numbers in the 300s earlier, I am going to start with 400 and go up by 100 until I get no results and then narrow by 10s and then 1s until I know where to stop ... and it appears we stop at 433.\n",
    "\n",
    "Now let's go build, er, revise us some code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "baseurl = \"http://www.libraries.wright.edu/special/dunbar/explore?book=*&id=\"\n",
    "mydirectory = \"/Users/jjl/Desktop/downloadedfiles/\"\n",
    "\n",
    "mylist = []\n",
    "for i in range (1, 434):\n",
    "    link = baseurl+str(i)\n",
    "    mylist.append(link)\n",
    "\n",
    "for link in mylist:\n",
    "    remotefile = urllib.request.urlopen(link).read()\n",
    "    soup = BeautifulSoup(remotefile, \"lxml\")\n",
    "    div = soup.find('div', 'bookContain-right')\n",
    "    localfile = open(mydirectory+link.replace(baseurl, '')+\".html\",'wt')\n",
    "    localfile.write(str(div.encode('utf-8')))\n",
    "    localfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code works, and it returns only the contents of the desired `div`:\n",
    "\n",
    "    <div class=\"bookContain-right\">\n",
    "\n",
    "But the contents remain ugly. At the very least, some regex is needed to clean up some of the escaped characters: those that begin with a backslash. Perhaps better would be to use `html2text` to convert the documents to plain text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "baseurl = \"http://www.libraries.wright.edu/special/dunbar/explore?book=*&id=\"\n",
    "mydirectory = \"/Users/jjl/Desktop/downloadedfiles/\"\n",
    "\n",
    "mylist = []\n",
    "for i in range (1, 2):\n",
    "    link = baseurl+str(i)\n",
    "    mylist.append(link)\n",
    "\n",
    "for link in mylist:\n",
    "    remotefile = urllib.request.urlopen(link).read()\n",
    "    soup = BeautifulSoup(remotefile, \"lxml\")\n",
    "    div = soup.find('div', 'bookContain-right')\n",
    "    text = html2text.html2text(str(div))\n",
    "    localfile = open(mydirectory+link.replace(baseurl, '')+\".txt\",'wt')\n",
    "    localfile.write(str(text))\n",
    "    localfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Munging\n",
    "\n",
    "> **Data munging** or **data wrangling** is loosely the process of manually converting or mapping data from one \"raw\" form into another format that allows for more convenient consumption of the data with the help of semi-automated tools. This may include further munging, data visualization, data aggregation, training a statistical model, as well as many other potential uses. Data munging as a process typically follows a set of general steps which begin with extracting the data in a raw form from the data source, \"munging\" the raw data using algorithms (e.g. sorting) or parsing the data into predefined data structures, and finally depositing the resulting content into a data sink for storage and future use.\n",
    "\n",
    "-- [Wikipedia](https://en.wikipedia.org/wiki/Data_wrangling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look at one of Zach's files:\n",
    "\n",
    "!less ./texts/apa0598.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No matter what Zach has in mind for this data, we can be pretty sure that it does not include a lot of angle brackets and funkiness like `div class=\"SANSLINE\"`. (For the record, *funkiness* is a technical term in data munging. I'm serious. Go look it up.) Whatever Zach's next steps are, he is going to want to clean up the text. \n",
    "\n",
    "For this workshop, we are going to skip transforming this html into some kind of operable xml and focus on simply getting it into useful plain text. From there, Zach will be able to engage a number of automated processes which may be more, or less, interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "myfile = open('./texts/apa0598.htm', 'r')\n",
    "myhtml = myfile.read()\n",
    "mytext = BeautifulSoup(myhtml).text\n",
    "\n",
    "print(mytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we just need to clean up the entire folder!\n",
    "\n",
    "Again, a bit of automation goes a long way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Please note that this script is incomplete for now. \n",
    "# Feel free to use it as a basis for a script that works.\n",
    "\n",
    "filesIN = \"/Users/jjl/Desktop/filesIN/\"\n",
    "filesOUT = \"/Users/jjl/Desktop/filesOUT/\"\n",
    "\n",
    "postlist = os.listdir(filesIN)\n",
    "\n",
    "for post in postlist: \n",
    "    text = BeautifulSoup(open(filesIN+post), \"lxml\")\n",
    "    text.encode(\"utf-8\")\n",
    "    fout = open(filesOUT+post, \"w\")\n",
    "    fout.write(text.encode(\"utf-8\"))\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Mining with the NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Text mining** or **text analytics** (or those same phrases with *data* substituted for *text*) means a lot of things. It can mean simple textual statistics, building a concordance (or seeing key words in context), determining what words co-locate (collocate) with other words within a text or across texts, as well as what words co-occur with others across a number of texts (e.g., topic modeling).\n",
    "\n",
    "That is, **text mining** is scalable both in terms of *scope*, focusing on small or large patterns, and in terms of *range*, focusing on a single text or dozens, hundreds, thousands of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our next series of explorations, we are going to use Python's NLTK module to work at the smaller end of the scope:\n",
    "\n",
    "* create a corpus and view concordances, synonyms, collactions\n",
    "* focus on only a single text to see what we can learn about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Corpus Views\n",
    "\n",
    "A lot of our text mining can be done by treating all the texts as one large text file: collocations, concordances, and other textual features can all be explored within a global context. So, one of the common things that needs to get done is to create one text file with which to work.\n",
    "\n",
    "The way to do this in Python is to read a folder full of texts into a single and then to join those list items into a single text. This seems weird, but this particular way of doing things actually uses less processing power and memory. The good news is that compiling documents into a single list is also the form in which Python's topic modeling module, `gensim`, expects them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "file_list = glob.glob('./texts' + '/*.txt')\n",
    "\n",
    "mytexts = []\n",
    "for filename in file_list:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        mytexts.append(f.read().replace('\\n', ' '))\n",
    "\n",
    "alltexts = ''.join(mytexts)\n",
    "print(alltexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With our text now *one big text file*, we can mumble the magic incantations which lets us play with our text within the NLTK:\n",
    "\n",
    "(If you are ever curious, type `one big text file` into a search engine and saunter through the results.)\n",
    "\n",
    "**N.B.**: If you ever get the response that you are missing something from NLTK, simply run `nltk.download()` and download what you need using the GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "tokens = nltk.word_tokenize(alltexts)\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With that done, we can now begin to explore our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "myword = text.concordance(\"recommend\")\n",
    "print(myword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text.similar(\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text.common_contexts([\"husband\", \"wife\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Brief Excursus on Control Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Control logic** is what makes automation of processes possible. That is all. \n",
    "\n",
    "**Control logic** is entirely about you making the decisions and then writing those decisions down and then telling the script *Go do that while I grab a cup of coffee.*\n",
    "\n",
    "**Control logic** is nothing to be scared of. \n",
    "\n",
    "    ... Nothing of which to be scared? \n",
    "    ... Nothing that should scare you? \n",
    "    ----------------------------------------------------------------\n",
    "    LogicError                     Traceback (most recent call last)\n",
    "    <English-input-13-4c23f72ea457> in <module>(prepositions)\n",
    "    ----> syntax.prepositions()\n",
    "    \n",
    "If you can master prepositions, you can master **control logic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look through most of the scripting in this notebook, you will note that one kind of control logic dominates: the **for** loop. Its logic looks like this:\n",
    "\n",
    "```\n",
    "for thing in collection:\n",
    "    do this\n",
    "```\n",
    "\n",
    "Implicit in this command is that the `for` loop is going to keep doing `this` until it runs out of `things`, at which point it stops. No more control logic is required of us as writers. In most cases, the collection is a folder (or directory) of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's build a script that gets a list of word counts in a text, saves the counts to a CSV file. We will edit that CSV file to have a header in it, and then we will use the file to create a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "file_list = glob.glob('./texts/*.txt')\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You are going to see **`glob`** a lot in these scripts. There are a number of ways to get files in Python. I tend to use `glob` because I tend to keep the files with which I am working in a single directory. \n",
    "\n",
    "Our first task is to feed `for` the `file_list`. \n",
    "\n",
    "Remember, our logic is `for` **`thing`** `in` **`collection`**. \n",
    "\n",
    "In this instance **`thing`** is the item with which we want to do something within the `for` loop, and we get to choose the name. (It could be `thing`, if you like, but remember that choosing meaningful names is part of writing good code.) In this next bit of code, you'll note that I've chosen the names `fpath` and `f` for the two iterators. I like those two names because they are very straightforward: the former is short for *filepath* which is what the `glob` module is in fact compiling into a list for me and `f` is shorthand for *file* which is what I want regex to work on. They are also traditional in Python coding. (Remember, I'm a folklorist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for fpath in glob.glob(\"../assets/*.txt\"):\n",
    "    with open(fpath) as f:\n",
    "         fixed_text = re.sub(\"[^a-zA-Z']\", \" \" , f.read().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At this point, we haven't really told the `for` loop what to do with all this information. \n",
    "\n",
    "For now, to keep things simple, we are going to build a **dictionary**. \n",
    "\n",
    "In Python, a dictionary is simply a container for pairs of items, where one item is the key and the other the value. (When you look up a term in the dictionary, the term is the key that allows you to access the value you seek, the definition.) \n",
    "\n",
    "For this dictionary, we are going to use the filepath as the key and we are going to pair it with a pair of numbers. (The pair of numbers is in a particular data structure known as a tuple, but let's leave the larger discussion of data structures for another time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "\n",
    "for fpath in glob.glob(\"./texts/*.txt\"):\n",
    "    with open(fpath, encoding=\"utf-9\") as f:\n",
    "         fixed_text = re.sub(\"[^a-zA-Z']\", \" \" , f.read().lower())\n",
    "    files[fpath] = [len(fixed_text.split()),len(set(fixed_text.split()))]\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Having compiled our texts and data points into a dictionary, we are close to done. We could, if we wanted to, simply run this script and grab the data from our console, or we could save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"textStats.csv\", \"w\") as fileOut:\n",
    "    for fname in files:\n",
    "        print(\"{},{},{}\".format(fname, files[fname][0], files[fname][1]), file=fileOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This time we have placed a `for` loop within a `with` statement. The `for` loop iterates over our dictionary, `files`, printing first the file name, which is the key, and then using the standard way of getting values out of a dictionary: dictionary[key]. Because our value is a list and we want one value then the other, and not both, we use a list index, 0 for the first value and 1 for the second value, to place them in the proper sequence as we build each line. \n",
    "\n",
    "With this final step, we have a CSV file that we can import into a spreadsheet application for further analysis and/or visualization. \n",
    "\n",
    "Here's the completed script, commented to make it easy to port elsewhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "\"\"\"textstats.py: basic statistics for a collection of text files\"\"\"\n",
    "\n",
    "import glob, re # import the two modules needed\n",
    "\n",
    "files = {} # create an empty dictionary which we will populate with the for loop\n",
    "\n",
    "# this for loop navigates to a directory, opens each file, normalizes the text\n",
    "# and then counts words, saving the counts to a list \n",
    "for fpath in glob.glob(\"./texts/*.txt\"):\n",
    "    with open(fpath, encoding=\"utf-8\") as f:\n",
    "         fixed_text = re.sub(\"[^a-zA-Z']\", \" \" , f.read().lower())\n",
    "    files[fpath] = [len(fixed_text.split()),len(set(fixed_text.split()))]\n",
    "\n",
    "# save the results to a file\n",
    "with open(\"textStats.csv\", \"w\") as fileOut:\n",
    "    for fname in files:\n",
    "        print(\"{},{},{}\".format(fname, files[fname][0], files[fname][1]), file=fileOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's Graph That"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"textStats.csv\", \",\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's a very tidy setup with very little work on our part. One line of code. Sorting by the longest text is just as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "by_longest = df.sort_values(by=\"Count\", axis=0, ascending=False) \n",
    "print(by_longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "ax = by_longest[['Count','Lexicon']].plot(kind='bar', \n",
    "                                           title =\"Text Stats\",\n",
    "                                           figsize=(20,10),\n",
    "                                           legend=True)\n",
    "ax.set_xlabel(\"Text\")\n",
    "ax.set_ylabel(\"Word Count\")\n",
    "ax.set_xticklabels(list(by_longest['Text'])) \n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "\"\"\"pytaChart.py: a bar chart from a dataframe of text statistics\"\"\"\n",
    "\n",
    "import pandas as pd, matplotlib as mpl\n",
    "\n",
    "# Import data from file into dataframe\n",
    "df = pd.read_csv(\"textStats.csv\", \",\")\n",
    "\n",
    "# Sort by the longest text\n",
    "by_longest = df.sort_values(by=\"Count\", axis=0, ascending=False) \n",
    "\n",
    "# Draw the graph (note use of ggplot)\n",
    "mpl.style.use('ggplot')\n",
    "ax = by_longest[['Count','Lexicon']].plot(kind='bar', \n",
    "                                           title =\"Text Stats\",\n",
    "                                           figsize=(15,10),\n",
    "                                           legend=True,\n",
    "                                           fontsize=12)\n",
    "ax.set_xlabel(\"Text\",fontsize=12)\n",
    "ax.set_ylabel(\"Word Count\",fontsize=14)\n",
    "ax.set_xticklabels(list(by_longest['Text'])) \n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Single Text Views\n",
    "\n",
    "We can also work with single texts to understand them in new ways. Here we will work with a short story, \"The Most Dangerous Game,\" published in time for it to be both in the public domain and yet remain a part of our collective literary/semiotic tradition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# First, let's load the text file:\n",
    "mdg = open(\"./mdg.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Next, let's find out how long it is:\n",
    "len(mdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 44078 what? Characters. We prefer words:\n",
    "import nltk\n",
    "mdgtokens = nltk.word_tokenize(mdg)\n",
    "len(mdgtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 9717 words AND punctuation marks. \n",
    "\n",
    "import re\n",
    "\n",
    "mdg_words = re.sub(\"[^a-zA-Z'-]\",\" \", open(\"./mdg.txt\").read())\n",
    "print(mdg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(mdg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mdg_word_list = mdg_words.split()\n",
    "print(mdg_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(mdg_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sorted(mdg_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sorted(set(mdg_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(sorted(set(mdg_word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "mdg2 = open(\"./mdg.txt\").read()\n",
    "mdg2_words = re.sub(\"[^a-zA-Z'-]\",\" \", open(\"./mdg.txt\").read().lower())\n",
    "mdg2_word_list = mdg2_words.split()\n",
    "len(sorted(set(mdg2_word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Lexical Diversity of MDG:\n",
    "len(mdg2_word_list) / len(set(mdg2_word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also use the NLTK to explore a single text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "\n",
    "mdg_raw = open(\"./mdg.txt\").read()\n",
    "mdg_words = re.sub(\"[^a-zA-Z'-]\",\" \", mdg_raw)\n",
    "mdg_case = mdg_words.lower()\n",
    "\n",
    "# print(mdg_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mdg_tokens = nltk.word_tokenize(mdg_case)\n",
    "mdg_text = nltk.Text(mdg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(mdg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(mdg_tokens) / len(set(mdg_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On average, a word occurs four times in \"The Most Dangerous Game.\"\n",
    "\n",
    "Out of curiosity, how many words occur four times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wordfrequency = nltk.FreqDist(mdg_tokens)\n",
    "four_times = [word for word in wordfrequency.keys() if wordfrequency[word] == 4]\n",
    "print(four_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mdg_text.count(\"dangerous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mdg_text.concordance(\"dangerous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Where does \"dangerous\" occur within the larger text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mdg_text.dispersion_plot([\"dangerous\", \"danger\", \"game\", \"fear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wordfrequency.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "freq_dic = {}\n",
    "for word in mdg_tokens:\n",
    "#        word = punctuation.sub(\"\", word)\n",
    "        try:\n",
    "            freq_dic[word] += 1\n",
    "        except: \n",
    "            freq_dic[word] = 1\n",
    "word_list = [(val, key) for key, val in freq_dic.items()]\n",
    "\n",
    "# print(sorted(word_list, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More useful to save this to a file:\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('output.csv', 'w') as f:\n",
    "        wtr = csv.writer(f)\n",
    "        wtr.writerows(sorted(word_list, reverse=True))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Mining from Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All text mining is simply an attempt to find numerical representations of texts that are meaningful/useful to the analyst. Nothing more. If there is magic in it, then it is in the **speed** and with which it can be done and the **scale** on which it can be come. \n",
    "\n",
    "There are a lot of possibilities, but the most popular one at present is known as **topic modeling**. The mechanics of topic modeling are so instrumental in the present moment that one form of it, **LSA** (Latent Semantic Analysis) is built right into Mac OS X.\n",
    "\n",
    "That noted the topic modeling that most humanists are doing is with Mallet, which uses a significant statistical tweak: **LDA** (Latent Dirichlet Allocation). \n",
    "\n",
    "No matter what form of topic modeling is in play, the process groups words by how often they occur with other words within a text, and, then, sorts the documents involved by these word groups. (The word groupings are called topics.)\n",
    "\n",
    "**Nota bene**: Topic models, like other statistical approaches to texts, do not pay any attention to syntax: texts are reduced to \"bags of words\": it is not unknown to re-create texts as `bow`s from word frequency distributions. (Topic models don't care.)\n",
    "\n",
    "*TL;DR: (1) The math works. (2) Cognitive studies suggest it captures some essential elements of how we think about texts. (3) Make sure you choose the math that is right for your work.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "PÃ©rez, Fernando and Brian E. Granger. 2007. IPython: A System for Interactive Scientific Computing. _Computing in Science and Engineering_ 9(3): 21-29. doi:10.1109/MCSE.2007.53. URL: http://ipython.org"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
